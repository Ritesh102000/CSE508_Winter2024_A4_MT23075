{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from rouge import Rouge\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_use_cpu = torch.device('cpu')\n",
    "device_use_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('Reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "\n",
    "df['training'] = df['Text'].str.lower()  + 'TL;DR' + df['Summary'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Summary','Text','training']][:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df['training'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device_use_cpu)\n",
    "optimizer = optimizer = optim.Adam(model.parameters(), lr=3e-4, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24811, 26, 7707, 220]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\" TL;DR \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reviews, test_reviews = train_test_split(df, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_len = len(tokenizer.encode(\" TL;DR \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncodedDataset(Dataset):\n",
    "    def __init__(self, tokenizer, rev, max_len):\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.eos = self.tokenizer.eos_token\n",
    "        self.eos_id = self.tokenizer.eos_token_id\n",
    "        self.rev = rev\n",
    "        self.result = []\n",
    "\n",
    "        for review in self.rev:\n",
    "            # Encode the text using tokenizer.encode(). We add EOS at the end\n",
    "            tokenized = self.tokenizer.encode(review + self.eos)\n",
    "\n",
    "            # Padding/truncating the encoded sequence to max_len\n",
    "            padded = self.pad_truncate(tokenized)\n",
    "\n",
    "            # Creating a tensor and adding to the result\n",
    "            self.result.append(torch.tensor(padded))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.result)\n",
    "\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.result[item]\n",
    "\n",
    "    def pad_truncate(self, name):\n",
    "        name_len = len(name) - extra_len\n",
    "        if name_len < self.max_len:\n",
    "            difference = self.max_len - name_len\n",
    "            result = name + [self.eos_id] * difference\n",
    "        elif name_len > self.max_len:\n",
    "            result = name[:self.max_len + 3]+[self.eos_id]\n",
    "        else:\n",
    "            result = name\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1203 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "train_dataset = EncodedDataset(tokenizer, train_reviews, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_save_modal(model, optimizer, dl, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for idx, batch in enumerate(dl):\n",
    "             with torch.set_grad_enabled(True):\n",
    "                optimizer.zero_grad()\n",
    "                batch = batch.to(device_use_cpu)\n",
    "                output = model(batch, labels=batch)\n",
    "                loss = output[0]\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if idx % 100 == 0:\n",
    "                    print(\"loss: %f, %d\"%(loss, idx))\n",
    "    torch.save(model.state_dict(), 'trained_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 6.585080, 0\n"
     ]
    }
   ],
   "source": [
    "train_save_modal(model=model, optimizer=optimizer, dl=train_dataloader, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_top_tokens(probabilities, num_tokens=9):\n",
    "    # The scores are initially softmaxed to convert to probabilities\n",
    "    probabilities = torch.softmax(probabilities, dim=-1)\n",
    "\n",
    "    # PyTorch has its own topk method, which we use here\n",
    "    token_probs, top_indices = torch.topk(probabilities, k=num_tokens)\n",
    "\n",
    "    # The new selection pool (9 choices) is normalized\n",
    "    token_probs = token_probs / torch.sum(token_probs)\n",
    "\n",
    "    # Send to CPU for numpy handling\n",
    "    token_probs = token_probs.cpu().detach().numpy()\n",
    "\n",
    "    # Make a random choice from the pool based on the new prob distribution\n",
    "    choice = np.random.choice(num_tokens, 1, p=token_probs)\n",
    "    token_id = top_indices[choice][0]\n",
    "\n",
    "    return int(token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, initial_text, max_length=15):\n",
    "    # Preprocess the initial text\n",
    "    initial_tokens = tokenizer.encode(initial_text)\n",
    "    generated_tokens = initial_tokens\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # Convert the current tokens into a tensor\n",
    "            input_ids = torch.tensor([generated_tokens]).to(device_use_cpu)\n",
    "\n",
    "            # Feed the tokens to the model to get predictions\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs.logits[0, -1]\n",
    "\n",
    "            # Select the next token based on top-k sampling\n",
    "            next_token_id = select_top_tokens(logits)\n",
    "\n",
    "            # If the chosen token is EOS, return the generated text\n",
    "            if next_token_id == tokenizer.eos_token_id:\n",
    "                return tokenizer.decode(generated_tokens)\n",
    "\n",
    "            # Append the new token to the generated text\n",
    "            generated_tokens.append(next_token_id)\n",
    "\n",
    "    # If no EOS token is generated, return after reaching max_length\n",
    "    return tokenizer.decode(generated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rouge_scores(generated_text, original_text):\n",
    "    rouge_evaluator = Rouge()\n",
    "\n",
    "    # Calculate ROUGE scores for the generated summary and the original review\n",
    "    rouge_scores = rouge_evaluator.get_scores(generated_text, original_text)\n",
    "\n",
    "    # Extract precision, recall, and F1 score for ROUGE-1\n",
    "    rouge1_p = rouge_scores[0]['rouge-1']['p']\n",
    "    rouge1_r = rouge_scores[0]['rouge-1']['r']\n",
    "    rouge1_f = rouge_scores[0]['rouge-1']['f']\n",
    "\n",
    "    # Extract precision, recall, and F1 score for ROUGE-2\n",
    "    rouge2_p = rouge_scores[0]['rouge-2']['p']\n",
    "    rouge2_r = rouge_scores[0]['rouge-2']['r']\n",
    "    rouge2_f = rouge_scores[0]['rouge-2']['f']\n",
    "\n",
    "    # Extract precision, recall, and F1 score for ROUGE-L\n",
    "    rougeL_p = rouge_scores[0]['rouge-l']['p']\n",
    "    rougeL_r = rouge_scores[0]['rouge-l']['r']\n",
    "    rougeL_f = rouge_scores[0]['rouge-l']['f']\n",
    "\n",
    "    return rouge1_p, rouge1_r, rouge1_f, rouge2_p, rouge2_r, rouge2_f, rougeL_p, rougeL_r, rougeL_f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for review in test_reviews:\n",
    "    print(\"Original Review: \", review)\n",
    "\n",
    "    # Generate summary for the current review\n",
    "    summary = generate_text(model, tokenizer, review + \" TL;DR \").split(\" TL;DR \")[5].strip()\n",
    "\n",
    "    # Print the summary\n",
    "    print(\"Generated Summary: \", summary)\n",
    "\n",
    "    # Calculate ROUGE scores\n",
    "    rouge1_p, rouge1_r, rouge1_f, rouge2_p, rouge2_r, rouge2_f, rougeL_p, rougeL_r, rougeL_f = calculate_rouge_scores(summary, review)\n",
    "\n",
    "    # Print ROUGE scores\n",
    "    print(\"ROUGE-1:\")\n",
    "    print(\"Precision:\", rouge1_p)\n",
    "    print(\"Recall:\", rouge1_r)\n",
    "    print(\"F1 Score:\", rouge1_f)\n",
    "\n",
    "    print(\"\\nROUGE-2:\")\n",
    "    print(\"Precision:\", rouge2_p)\n",
    "    print(\"Recall:\", rouge2_r)\n",
    "    print(\"F1 Score:\", rouge2_f)\n",
    "\n",
    "    print(\"\\nROUGE-L:\")\n",
    "    print(\"Precision:\", rougeL_p)\n",
    "    print(\"Recall:\", rougeL_r)\n",
    "    print(\"F1 Score:\", rougeL_f)\n",
    "\n",
    "    # Break the loop after processing one review\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "def train_with_evaluation(net, tknzr, train_set, lr, b_size, num_epochs):\n",
    "    # Split data into training and validation\n",
    "    dataset = EncodedDataset(tknzr, train_set, max_len)\n",
    "    train_sz = int(0.8 * len(dataset))\n",
    "    val_sz = len(dataset) - train_sz\n",
    "    train_set, val_set = random_split(dataset, [train_sz, val_sz])\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=b_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=b_size, shuffle=False)\n",
    "\n",
    "    # Setup model, optimizer, and move model to device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net = net.to(device)\n",
    "    optimizer = torch.optim.AdamW(net.parameters(), lr=lr)\n",
    "\n",
    "    # Training loop\n",
    "    net.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            outputs = net(input_ids=batch, labels=batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    # Validation loop\n",
    "    net.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = batch.to(device)\n",
    "            outputs = net(input_ids=batch, labels=batch)\n",
    "            loss = outputs.loss\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    return avg_val_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [3e-4, 1e-4]\n",
    "batch_sizes = [8, 16]\n",
    "num_epochs = [5, 7]\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_hyperparams = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        for epochs in num_epochs:\n",
    "            average_val_loss = train_with_evaluation(model, tokenizer, train_reviews, lr, bs, epochs)\n",
    "            print(f\"Validation Loss for LR={lr}, BS={bs}, Epochs={epochs}: {average_val_loss}\")\n",
    "            if average_val_loss < best_loss:\n",
    "                best_loss = average_val_loss\n",
    "                best_hyperparams = {'learning_rate': lr, 'batch_size': bs, 'num_epochs': epochs}\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_hyperparams)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
